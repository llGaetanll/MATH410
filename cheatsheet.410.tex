\documentclass[12pt,letterpaper]{article}

% \usepackage[margin=0.9in, showframe]{geometry}
\usepackage[
    top=5mm,
    bottom=5mm,
    left=5mm,
    right=5mm,
    marginparwidth=0mm,
    marginparsep=0mm,
    headheight=15pt,
    centering,
    % showframe,
    includefoot,
    includehead
    ]{geometry}


\usepackage{setspace}
\usepackage{titlesec}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{parskip}


\usepackage{blindtext}
\usepackage{multicol}
\usepackage{fancyhdr}

% add more margin between multicols columns
\setlength\columnsep{10pt} 

% remove all paragraph indents
\setlength{\parindent}{0pt}


\vspace{-\baselineskip}

% ================== %
% format of sections
% ================== %
\titleformat{\section}
   {\small\sc\bfseries\MakeUppercase}{***}{0.2em}{\underline}
% \titlespacing{\section}{-1pc}{*4}{*1.5}
\titlespacing{\section}{0pc}{*4}{*1.5}

\titleformat{\subsection}
   {\normalsize\sc\bfseries}{**}{0.2em}{\underline}
\titlespacing{\subsection}{0pc}{*4}{*1.5}

\titleformat{\subsubsection}
   {\normalsize\sc}{}{1em}{\underline}
% \titlespacing{\subsection}{-0.5pc}{*4}{*1.5}
\titlespacing{\subsubsection}{-1pc}{*4}{*1.5}



% ================= %
% Headers & Footers
% ================= %
\pagestyle{fancy}
\fancyhf{}
\lhead{\bf MATH410 Cheatsheet}
\rhead{Gaetan Almela}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}



% ================= %
% Aliases
% ================= %
\def\cov{\text{Cov}}
\def\var{\text{V}}
\def\ev{\text{E}}
\def\eps{\varepsilon{}}
\def\Dom{\text{Dom}}



% ================= %
% Commands
% ================= %
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\D}{{\mathbb D}}
\newcommand{\N}{{\mathbb N}}

\newcommand{\btw}[1]{
    $\langle$ #1 $\rangle$
}

% create a command for TODOs
\newcommand{\TODO}{\color{red}\textbf{TODO}\color{black}}


% create a command to display examples in a box
\newcommand{\ex}[1]{
    \medskip \noindent
    \fcolorbox{lightgray}{white}{
        \parbox{0.8\linewidth}{
            {\bf Example} \\

            #1
        }
    } 
}


\begin{document}
    \footnotesize
    \begin{multicols*}{2}
        \section{Induction}

        {\bf Base Case}

        Prove $S(n_0)$ \\

        {\bf Inductive Hypothesis}

        Assume $S(n)$ holds for an \underline{arbitrary} $n \geq n_0$ \\

        {\bf Inductive Step}

        Given $S(n)$, prove $S(n + 1)$

        \subsubsection{Axiom of Completeness}

        If $S$ has an upper bound, then it has a {\it least} upper bound, namely
        the supremum of $S$.

        Likewise if $S$ has a lower bound, then it has a {\it greatest} lower
        bound, namely the infimum of $S$.

        \section{Archimedean Principle}

        If $c$ is any positive number, then there is an integer $n > c$,
        equivalently

        $\forall x \in \R, \exists n \in \N \text{ such that } n > x$ \\

        Additionally, if $c$ is any positive number, then there is an integer
        $n$ with $0 < \frac{1}{n} < c$, equivalently

        $\forall \eps > 0, \exists n \in \N \text{ such that } \frac{1}{n} < \eps$ \\

        \subsubsection{Triangle Inequality}

        \[
            |a + b| \leq |a| + |b|
        \]

        for all $a, b \in \R$. The following can also be proven from the
        Triangle Inequality.

        \begin{align*}
            |a| - |b| &\leq |a + b| \\
            ||a| - |b|| &\leq |a - b| \\
            |a| - |b| &\leq |a - b| \\
        \end{align*}

        \subsubsection{Misc Binomial Formulas}

        \begin{align*}
            a^n - b^n &= (a - b)\left( \sum_{i = 1}^{n} a^{n - i}b^{i - 1} \right) \\
                      &= (a - b)\left( a^{n - 1} + a^{n - 2}b + \cdots + ab^{n - 2} + b^{n - 1} \right)
        \end{align*}
        
        for all $a, b \in \R$ with $n \geq 2$.

        {\bf Factoring $a^n + b^n$}

        This is only possible if $n$ is {\bf odd}.

        {\bf Factoring $1 - r^n$}

        \begin{align*}
            1 - r^n &= (1 - r)\left( \sum_{k = 0}^{n - 1} r^k \right) \\
                    &= (1 - r)\left( 1 + r + r^2 + \cdots + r^{n - 1} \right)
        \end{align*}
        
        If $r \neq 1$, then this is a {\bf geometric sum} and the following is true

        \[
            \sum_{k = 0}^{n - 1} r^k = \frac{1 - r^k}{1 - r}
        \]

        {\bf Binomial Formula}

        Recall

        \[
            {}_nC_k = \binom{n}{k} = \frac{n!}{k!(n - k)!}
        \]

        for $0 \leq k \leq n$. \\

        Now

        \begin{align*}
             &(a + b)^n \\
            =&\sum_{k = 0}^{n} \binom{n}{k} a^{n - k}b^{k} \\
            =&\binom{n}{0}a^n + \binom{n}{1}a^{n - 1}b + \binom{n}{2}a^{n - 2}b^2 + \cdots + \binom{n}{n}b^n \\ 
        \end{align*}

        {\bf Consequence}

        Let $\frac{p}{q} \in \Q$ be a solution to

        \[
            a_nx^n + a_{n - 1}x^{n - 1} + \cdots + a_1x + a_0 = 0
        \]

        with $a_i \in \Z$ for all $0 \leq i \leq n$. Then $p | a_0$ and $q |
        a_n$

        {\bf Proof}

        \[
            a_n\left(\frac{p}{q}\right)^n + a_{n - 1}\left(\frac{p}{q}\right)^{n - 1} + \cdots + a_1\left(\frac{p}{q}\right) + a_0  = 0
        \]

        Multiplying by $q^n$, we get

        \begin{align*}
            a_np^n + a_{n - 1}p^{n - 1}q + \cdots + a_1pq^{n - 1} + a_0q^n  = 0
        \end{align*}

        With some manipulation, we get

        \begin{align*}
            a_np^n&= - \left(a_{n - 1}p^{n - 1}q + \cdots + a_0q^n \right) \\
                  &= -q\left(a_{n - 1}p^{n - 1} + \cdots + a_0q^{n - 1} \right)
        \end{align*}

        so $q | a_np^n$ and thus $q | a_n$.

        A similar manipulation from (1) can be done with $a_0q^n$ to obtain

        \begin{align*}
            a_0q^n&= - \left(a_{n}p^{n} + \cdots + a_1pq^{n - 1} \right) \\
                  &= -p\left(a_{n - 1}p^{n - 1} + \cdots + a_0p^{n - 1}q^{n - 1} \right)
        \end{align*}

        so $p | a_0q^n$ and thus $p | a_0$.

        {\bf Application}

        Does $2x^2 - 5x - 3 = 0$ have a rational solution $\frac{p}{q}$ ?

        If so, $p | -3$ so $p = \pm 1$ or $p = \pm 3$ and $q | 2$ so $q = \pm
        1$ or $q = \pm 2$.

        \section{Monotone Convergence Theorem}

        Let $\{a_n\}_{n = 1}^{\infty}$ be monotone.

        $\{a_n\}_{n = 1}^{\infty}$ converges {\bf if and only if} it is bounded.

        \subsection{Nested Interval Theorem}

        Let $I_n = [a_n, b_n]$ with $I_n \subseteq I_{n + 1}$ for $n \ge 1$.

        If $\lim_{n \to \infty} (b_n - a_n) = 0$.

        \btw{We need this condition to ensure the interval converges to a
        point, and not a sub-interval} \\

        Then there is a unique $x^*$ in $\bigcap_{n = 1}^{\infty} I_n = \{x : x
        \in I_n \text{ for all $n$ } \}$

        \btw{There is a unique point $x^*$ that is inside all intervals $I_n$} \\

        and $\lim_{n \to \infty} a_n = \lim_{n \to \infty} b_n = x^*$.

        \btw{$a_n$ and $b_n$ converge to the same value: $x^*$}

        \TODO{} Boundedness, Sum Rule, Product Rule, Quotient Rule,
        Convergence...

        \subsubsection{Subsequences}

        Let $\{a_n\}_{n = 1}^{\infty}$ be a sequence and let $\{n_k\}_{k =
        1}^{\infty}$ be a sequence of positive integers. Then, $\{a_{n_k}\}$ is
        a subsequence of $\{a_n\}_{n = 1}^{\infty}$.

        {\bf Note}: A subsequence is infinite.

        \subsubsection{Peak Index}

        A peak index $n_*$ for a sequence $\{a_n\}_{n = 1}^{\infty}$ is such
        that $n_* \ge 1$ with $a_m \le a_{n_*}$ for all $m \ge n_*$.

        \btw{All values that come {\bf after} value $n_*$ are less than or equal to
        it.}

        \subsubsection{Every sequence has a monotone subsequence}

        \begin{enumerate}
            \item If $\{a_n\}$ has infinitely many peak indices,

                \btw{for example $\{ \frac{1}{n} \}$}

                then the subsequence $\{a_{n_k}\}$ with $\{n_k\}_{k =
                1}^{\infty}$ is a decreasing subsequence.

            \item If $\{a_n\}$ has finitely many peak indices,

                \btw{ for example $\{n\}$}

                then there is a subsequence that is strictly increasing.
        \end{enumerate}

        \subsection{Corollary 2.33}
        Every bounded sequence $\{a_n\}$ has a convergent monotone subsequence.

        \begin{enumerate}
            \item If $\{a_n\}$ has infinitely many peak indices, the
                subsequence is decreasing.

            \item If $\{a_n\}$ has finitely many peak indices, the subsequence
                is also decreasing.
        \end{enumerate}

        Then, you can use the Monotone Convergence Theorem so show that this
        subsequence converges.

        \subsubsection{Sequential Compactness}

        Let $S \subseteq \R$ be a set. $S$ is sequentially compact if every
        sequence has a subsequence that converges.

        Any set $[a, b] \subseteq \R$ is sequentially compact.

        \subsubsection{Density in $\R$}

        A set $S$ is dense in $\R$ if and only if for each $x \in \R$, there is
        a sequence $\{a_n\}_{n = 1}^{\infty} \subseteq S$ with $a_n \rightarrow x$

        \subsubsection{Sequential Continuity}

        Let $f: D \rightarrow \R$ be a function. $f$ is continuous at $x_0 \in
        D$ if, for any sequence $\{x_n\}_{n = 1}^{\infty} \subseteq D$ with $x_n
        \rightarrow x_0$, then $f(x_n) \rightarrow f(x_0)$.

        \btw {
            $f$ is continuous at $x_0$ if, for all sequences subset of the domain
            that converge to $x_0$, applying the $f$ to the sequence converges to
            $f(x_0)$
        }

        \subsubsection{Rules of Continuity}

        Let $f, g$ be continuous functions at $x_0 \in D$.

        \begin{itemize}
            \item $f + g$ is continuous at $x_0$
            \item $f \cdot g$ is continuous at $x_0$
            \item If $g(x_0) \neq 0$, then $\frac{f}{g}$ is continuous at $x_0$
        \end{itemize}

        \subsubsection{Maximum Value}
        $f: D \rightarrow \R$ has a maximum value $f(x_0)$ if $x_0 \in D$ and
        $f(x) \le f(x_0)$ for all $x \in D$. There is only ever {\bf at most
        one} maximum value.

        \subsubsection{Minimum Value}
        $f: D \rightarrow \R$ has a minimum value $f(x_0)$ if $x_0 \in D$ and
        $f(x) \ge f(x_0)$ for all $x \in D$. There is only ever {\bf at most
        one} minimum value.

        \subsubsection{Extreme Values}
        Extreme values are the maximum and minimum values of $f$

        \subsubsection{Maximizer}
        $x_0$ is a maximizer of $f$ if $f(x_0)$ is the maximum value of $f$.
        There can be {\bf infinitely many} maximizers. 

        \ex {
            Let $f(x) = sin(x)$. Then $\frac{\pi}{2} + 2 \pi n$ are all the
            maximizers of $f$, and the maximum value is $1$.
        }

        \subsubsection{Minimizer}
        $x_0$ is a minimizer of $f$ if $f(x_0)$ is the minimum value of $f$.
        There can be {\bf infinitely many} minimizers.

        \ex {
            Let $f(x) = sin(x)$. Then $-\frac{\pi}{2} + 2 \pi n$ are all the
            minimizers of $f$, and the minimum value is $-1$.
        }

        \ex {
            Let $f(x) = x$ with $0 < x < 1$. Then $f$ has no maximum nor
            minimum values.
        }

        \subsection{Lemma 3.10}

        If $f: [a, b] \rightarrow \R$ is continuous, then it is bounded.

        \section{Extreme Value Theorem}

        Let $f: [a, b] \rightarrow \R$ be continuous. Then, $f$ has a maximum
        value, and a minimum value.

        \section{Intermediate Value Theorem}

        Let $f: [a, b] \rightarrow \R$ be continuous. Let $c \in [f(a), f(b)]$
        be arbitrary, with $f(a) < c < f(b)$.

        Then there is an $x_0 \in [a, b]$ such that $f(x_0) = c$.

        {\bf Note:} If $f$ is a polynomial of odd degree, then the range of $f$
        is $(-\infty, \infty)$.

        \subsubsection{Bisection Method}

        This is used to approximate the zeroes of a function $f$ that is
        continuous on $[a, b]$, with $f(a)f(b) < 0$.

        Let $c_1 = \frac{a + b}{2}$ be the midpoint of $[a, b]$. If $f(c_1) =
        0$, then we are done.

        If $f(c_1)f(a) > 0$, let $c_2 = \frac{a + c_1}{2}$, and repeat.

        If $f(c_1)f(b) > 0$, let $c_2 = \frac{c_1 + b}{2}$, and repeat.

        If $b - a = 1$, then $c_n$ is within $\frac{b - a}{2^n}$ of a zero of
        $f$.

        \section{Uniform Continuity}

        The function $f: D \rightarrow \R$ is {\it uniformly continuous} on D
        if, whenever $\{u_n\}_{n = 1}^{\infty}$ and $\{v_n\}_{n = 1}^{\infty}$
        are sequences in $D$ such that $|u_n - v_n| \rightarrow 0$ as $n
        \rightarrow \infty$, then $|f(u_n) - f(v_n)| \rightarrow 0$ as $n
        \rightarrow \infty$.

        {\bf Note}: If $f$ is uniformly continuous on $D$, then it is continuous
        on $D$.

        {\bf Note}: Neither $\{u_n\}_{n = 1}^{\infty}$ nor $\{v_n\}_{n =
        1}^{\infty}$ need to be convergent, or even bounded.


        \subsection{Theorem 3.17}

        Let $f: [a, b] \rightarrow \R$ be continuous. Then $f$ is uniformly
        continuous.

        \subsubsection{$\eps$ -- $\delta$ Continuity}

        For all $\eps > 0$, there exists $\delta > 0$ such that, if $0 < |x - a|
        < \delta$ then $|f(x) - f(a)| < \eps$.

        This definition is equivalent to the sequential definition of
        continuity.

        \subsubsection{Theorem 3.20}

        The sequence definition of continuity is equivalent to the $\eps -
        \delta$ definition.

        \subsubsection{Monotone Functions}

        $f: D \rightarrow \R$ is {\bf monotonically increasing} if for any $x, z
        \in D$, $x < z \Rightarrow f(x) \le f(z)$.

        $f: D \rightarrow \R$ is {\bf strictly monotonically increasing} if for any $x, z
        \in D$, $x < z \Rightarrow f(x) < f(z)$.

        $f: D \rightarrow \R$ is {\bf monotonically decreasing} if for any $x, z
        \in D$, $x < z \Rightarrow f(x) \ge f(z)$.

        $f: D \rightarrow \R$ is {\bf strictly monotonically decreasing} if for any $x, z
        \in D$, $x < z \Rightarrow f(x) > f(z)$.


        \subsubsection{Thm 3.23}

        Let $f$ be monotone and $f(D)$ is an {\bf interval} \btw{no unions, must
        be a contiguous interval}. Then $f$ is continuous.

        \subsubsection{Injective Functions}

        $f: D \rightarrow \R$ is {\bf injective} \btw{one to one} if for any $x,
        z \in D$, $x \ne z \Rightarrow f(x) \ne f(z)$.

        \subsubsection{Surjective Functions}

        $f: D \rightarrow \R$ is {\bf surjective} \btw{onto} if for all $y \in
        \R$, there exists $x \in D$ such that $f(x) = y$. \btw{Note that $x$ may
        not be unique.}

        \subsubsection{Inverse Functions}

        Let $f: D \rightarrow \R$ be one to one. Then $f$ has an inverse
        $f^{-1}$, defined by

        \[
          f^{-1}(y) = x \Leftrightarrow f(x) = y
        \]

        \subsubsection{Limit Points}

        If $D \subseteq \R$, then $x_0$ is a {\bf limit point} of $D$ if there
        exists a sequence $\{x_n\}_{n = 1}^{\infty} \subseteq D$ with $\{x_n\}
        \rightarrow x_0$ and $x_n \ne x_0$ for any $n$. \btw{The sequence
        approaches $x_0$, but is never $x_0$. }

        \subsubsection{Limits}

        Let $f: D \rightarrow \R$ and $x_0$ be a limit point of $D$. Then
        $\lim_{x \to x_0} f(x) = L$ if for each $\{x_n\} \subseteq D$ with $x_n
        \ne x_0$ for all $n$, then $x_n \rightarrow x_0$ {\it implies that}
        $f(x_n) \rightarrow L$. \btw{ The limit of all sequences $\{x_n\}$ must
        converge}

        \subsubsection{Composition of Limits}

        Let $f: D \rightarrow \R$ and $g: U \rightarrow \R$ with $f(D) \subseteq
        U$. Suppose that $x_0$ is a limit point of $D$, $\lim_{x \to x_0}
        f(x) = y_0$, and $\lim_{y \to y_0} g(y) = L$. Then $\lim_{x \to x_0}(g
        \circ f)(x) = L$.

        \TODO{} Quotient Rule, Product Rule, Sum Rule of limits.

        \newpage

        Chapter 4

        \subsubsection{Neighborhoods}

        If $x_0 \in I = (a, b)$, then $I$ is a {\bf neighborhood} of $x_0$.

        \subsubsection{The Derivative}

        Let $f: D \rightarrow \R$, $x_0 \in D$ and $D$ contains a neighborhood
        of $x_0$. If $\lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0}$ exists,
        then it is the {\bf derivative} of $f$ at $x_0$; it is denoted as
        $f'(x_0)$.

        We say that $f$ is differentiable at $x_0$. \btw { Likewise, if this is
        true of all points on an interval $I$, we say that $f$ is differentiable
        on $I$. }


        \subsection{Proposition 4.5}

        If $f'(a)$ exists, then $f$ is continuous at $a$.

        \subsection{Sum Rule}

        \[
          (f + g)'(x_0) = f'(x_0) + g'(x_0)
        \]

        \subsection{Product Rule}

        \[
          (fg)'(x_0) = f(x_0)g'(x_0) + f'(x_0)g(x_0)
        \]

        \subsection{Quotient Rule}

        If $g(x_0) \ne 0$, then

        \[
          \left(\frac{f}{g}\right)'(x_0) = \frac{f'(x_0)g(x_0) - f(x_0)g'(x_0)}{g^2(x_0)}
        \]

        \subsection{Chain Rule}

        Let $f: I \rightarrow \R$ and $g: J \rightarrow \R$ with $f(I) \subseteq
        J$. Additionally, assume that $f'(x_0), g'(f(x_0))$ exist. Then,

        \[
          (f \circ g)'(x_0) = [g'(f(x_0))]f'(x_0)
        \]

        \subsubsection{Rolle's Theorem}

        Let $f: A \rightarrow B$ be continuous on $[A, B]$ and differentiable on
        $(A, B)$. Further assume that $f(a) = f(b)$. Then, there exists an $x_0
        \in (a, b)$ with $f'(x_0) = 0$.

        \TODO{} check this definition

        \section{Mean Value Theorem}

        Let $f: [a, b] \rightarrow \R$ be continuous, and be such that $f'$
        exists on $(a, b)$. Then, there is an $x_0 \in (a, b)$ with 

        \[
          f'(x_0) = \frac{f(b) - f(a)}{b - a}
        \]

        \btw {There is a point in $(a, b)$ such that the tangent line at that
        point is parallel to a line from $(a, f(a)), (b, f(b))$}

        \btw {This is a generalization of Rolle's Theorem}

        \subsubsection{Lemma 4.19}

        \TODO{} Makes no sense in my notes

        \subsection{Identity Criterion}

        Let $I$ be an open interval, and $f: I \rightarrow \R$, $g: I
        \rightarrow \R$. Then, $f' = g'$ on $I$ {\bf if and only if} there is a
        constant $c$ with $f(x) = g(x) + c$, for all $x \in I$.


        \subsubsection{$2^{\text{nd}}$ Derivative Test}

        Let $I$ be an open interval, let $x_0 \in I$, and let $f: I \rightarrow
        \R$ be such that $f'$ and $f''$ exist on $I$, with the additional
        property that $f'(x_0) = 0$. Then

        \begin{itemize}
          \item $f''(x_0) < 0 \Rightarrow x_0$ is a local maximizer
          \item $f''(x_0) > 0 \Rightarrow x_0$ is a local minimizer
        \end{itemize}

        \subsubsection{Cauchy Mean Value Theorem}

        Let $f: [a, b] \rightarrow \R$ and $g: [a, b] \rightarrow \R$ be
        continuous on $[a, b]$ and differentiable on $(a, b)$, with $g'(x) \ne
        0$ for all $x$ and $g(b) \ne g(x)$. Then there is $x_0 \in (a, b)$ within

        \[
          \frac{f(b) - f(a)}{g(b) - g(a)} = \frac{f'(x_0)}{g'(x_0)}
        \]

        \subsubsection{Theorem 4.24}

        Let $I$ be open, $n \ne 1$, $f: I \rightarrow \R$ with $n^{\text{nt}}$
        derivative of $f$ existing on $I$. If $f^{(k)}(x_0) = 0$ for some $x_0
        \in I$, $0 \le k < n$, then for all $x \in I$ there is $z_x$ between $x$
        and $x_0$ with

        \[
          f(x) = \frac{f^{(n)}(z_n)}{n!}(x - x_0)^n
        \]

        \subsubsection{L'H\^opital's Rule}

        Let $f, g$ be continuous on $(a, b)$, $f'$, $g'$ exist on $(a, b)$
        assume $g(x) \ne 0$, also $g'(x) = 0$ for $x \in (a, b)$. Then,

        \[
          \lim_{x \to a^+} \frac{f(x)}{g(x)} = \lim_{x \to a^+}
          \frac{f'(x)}{g'(x)}
        \]

        If $f(a) = g(a) = 0$

        \subsubsection{Darboux Sums}

        Let $f: [a, b] \rightarrow \R$ be bounded and $P = \{a_1 = x_1, ..., x_n
        = b\}$ be a partition on $[a, b]$

        {\bf Lower Sum}

        \[
          L(f, P) = \sum_{i = 1}^n m_i(x_i - x_{i - 1})
        \]

        {\bf Upper Sum}

        \[
          U(f, P) = \sum_{i = 1}^n M_i(x_i - x_{i - 1})
        \]

        Where $M_i = \sup(x), m_i = \inf(x)$ for $x \in (x_{i - 1}, x_i)$.


        \subsubsection{Partition Refinment}

        A partition $P^*$ is a refinment of $P$ on $[a, b]$ if

        \[
          P \subseteq P^* \subset [a, b]
        \]

        {\bf Note}

        If $P^*$ is a refinment of $P$ on $[a, b]$, then for $f$ bounded on $[a,
        b]$, $L(f, P) \le L(f, P^*)$ and $U(f, P) \ge U(f, P^*)$.

        {\bf Note}

        If $f: [a, b] \rightarrow \R$ is continuous and $P$, $Q$ are partitions
        on $[a, b]$, then

        \[
          L(f, P) \le U(f, Q)
        \]

        \subsubsection{Lemma 6.3}

        Let $f: [a, b] \rightarrow \R$ be bounded. Let $P, Q$ be partitions on
        $[a, b]$. Then

        \[
          L(f, P) \le U(f, Q)
        \]


        \subsubsection{Upper and Lower Integrals}

        Let $f: [a, b] \in \R$ be bounded, then

        \begin{multicols}{2}
          \[
            \int_{_{-}a}^{b} f = \sup(L(f, P))
          \]

          \[
            \int_{a}^{^{-}b} f = \sup(L(f, P))
          \]
        \end{multicols}

        For a partition $P$

        $f$ is integrable on $[a, b]$ if

        \[
            \int_{_{-}a}^{b} f = \int_{a}^{^{-}b} f
        \]


        \section{Archimedes Riemann Theorem}

        Let $f: [a, b] \rightarrow \R$ be bounded. $f$ is integrable {\bf if and
        only if} there exists a sequence of partitions $\{P_n\}_{n =
        1}^{\infty}$ such that

        \[
          \lim_{n \to \infty} (U(f, P_n) - L(f, P_n)) = 0
        \]

        \btw {The difference of the upper and lower Darboux sums converges.}


        \subsubsection{Partition Gaps}

        The gap of a partition on $[a, b]$ is the largest subinterval $[x_{i -
        1}, x_i]$ of $P$.

        \subsubsection{Regular Partitions}

        A partition $P$ on $[a, b]$ is regular if all subuintervals have the same length

        \[
          h = \frac{b - a}{n} = x_i - x_{i - 1}
        \]

        with $h$ as the gap size.

        {\bf Note}

        \[
          \int_b^a f := - \int_a^b f
        \]

        \subsection{Integrability Theorem}

        Let $f: [a, b] \rightarrow \R$ be continuous. Let $G(x) \int_a^x f(t)dt$
        for $x \in [a, b]$. Then

        \[
          G'(x) = f(x)
        \]

        for $x \in (a, b)$

        {\bf Def}

        $G$ is the {\bf antiderivative} of $g$ if $G' = g$

        \section{Fundamental Thm of Calculus I}

        Let $F: [a, b] \to \R$ be continuous and let $f = F'$ on $(a, b)$ with
        $f$ continuous and bounded on the open interval. Then
        
        \[
          \int_a^b f(t) dt = \int_a^b F'(t) dt = F(b) - F(a)
        \]

        \section{Fundamental Thm of Calculus II}

        Let $f: [a, b] \rightarrow \R$ be contiuous. Then

        \[
          \frac{d}{dx} \int_a^x f(t) dt = f(x)
        \]

        for $x \in (a, b)$

        {\bf Corollary}

        Let $I, J$ be open intervals, let $f: I \rightarrow \R$, $\phi: J
        \rightarrow \R$ with $\phi(J) \subseteq I$. Assume that $\phi$ is
        differentiable and $f$ is continuous. Then

        \[
          \frac{d}{dx} \int_a^{\phi(x)} f(t) dt = [f(\phi(x))]\phi'(x)
        \]


        \subsubsection{Mean Value Thm for Integrals}

        Let $f: [a, b] \rightarrow \R$ be continuous. Then, there is an $x_0 \in
        [a, b]$ with

        \[
          \frac{1}{b - a} \int_a^b f(t) dt = f(x_0)
        \]

        or equivalently,

        \[
          \int_a^b f(t) dt = f(x_0)(b - a)
        \]

        \btw { You can turn the area into a rectangle. }

        \subsubsection{Order of Contact}

        Let $I$ be an open interval. Let $x_0 \in I$, and let $f: I \to \R$, $g:
        I \to \R$. Then $f, g$ have {\bf contact of order} $n$ at $x_0$ if

        \[
          f^{(k)}(x_0) = g^{(k)}(x_0) \text{ for } k = 0, ..., n
        \]

        \subsubsection{Taylor Polynomials}

        Let $x_0 \in I$ with $I$ open, and $f: I \to \R$ with derivatives at
        $x_0$. Then there is a unique polynomial $P_n$ with degree $P_n \le n$
        such that contact order with $f$ at $x_0$ is $n$. $P_n$ is called {\bf
        the $n^{th}$ Taylor Polynomial of $f$ at $x_0$}, and 

        \[
          P_n(x) = f(x_0) + \frac{f'(x_0)}{1!}(x - x_0) + \cdots + \frac{f^{(n)}(x_0)}{n!}(x - x_0)^n
        \]

        \section{Lagrange Remainder Formula}

        Let $f^{(n + 1)}$ exist on an open neighborhood $I$ for $x_0$. Then

        \begin{align*}
          f(x) =&P_n(x) + R_n(x) \\
               =&\left[ \sum_{k = 0}^n \frac{f^{(n)(x_0)}}{k!}(x - x_0)^k \right] + \frac{f^{(n + 1)}(c_x)}{(n + 1)!}(x - x_0)^{n + 1}
        \end{align*}

        Where $R_n$ is the remainder, and $c_x \in (x, x_0)$

        \subsubsection{Thm 8.4}

        Let $f: I \to \R$ with $I$ open, let $x_0 \in I$, and assume that
        $f^{(n)}(x_0)$ exists for all $n$. If there are $r > 0$ and $M < \infty$
        so that $|x - x_0| < r$, then $|f^{(n)}(x)| \le M^n$ for all $n \ge 0$,
        and $x \in I$. Then $\lim_{n \to \infty} R_n(x) = 0$ for such $x$ some

        \[
          f(x) = \sum_{k = 1}^{\infty} \frac{f^{(k)}(x_0)}{k!}(x - x_0)^k
        \]

        \subsubsection{Cauchy Sequence}

        A sequence is Cauchy if, for any $\eps > 0$, there exists an $N \in \N$
        such that, for any $n, m \in \N$, if $n, m > N$, then $|a_m - a_n| <
        \eps$.

        {\bf Thm 9.4}: A sequence converges if and only if it is Cauchy.

        \subsubsection{Convergence Tests}

        {\bf $k^{th}$ term test}

        If $\sum_{k = 1}^{\infty} a_k$ converges, then $\lim_{n \to \infty} a_k
        = 0$. \btw{Note that {\bf the converse is not always true}, i.e.
        Harmonic series.}


        {\bf Comparison Test}

        If $\sum_{k = 1}^{\infty} b_k$ converges and $b_k \ge a_k$, then
        $\sum_{k = 1}^{\infty} a_k$ converges.


        {\bf Geometric Series}

        Let $c \ne 0$, if $0 \le r < 1$, then

        $\sum_{k = 0}^{\infty} r^k = \frac{1}{1 - r}$, and $\sum_{k =
        m}^{\infty} r^k = r^m \sum_{k = 1}^{\infty} r^k = \frac{r^m}{1 - r}$


        $\sum_{k = 1}^{\infty} c r^k = \frac{cr^m}{1 - r}$ if $|r| < 1$.


        {\bf Ratio Test}

        Let $a_k > 0$ for all $k \ge 1$. Assume that $\lim_{k \to \infty}
        \frac{a_{k + 1}}{a_k} = L$. If $0 \ge L < 1$, then $\sum_{k =
        1}^{\infty} a_k$ converges. If $L > 1$, $\sum_{k = 1}^{\infty} a_k$
        diverges. If $L = 1$ either could happen.


        {\bf Integral Test}

        Let $\{a_k\}_{k = 1}^{\infty}$ be a strictly decreasing sequence, and
        $\lim_{k \to \infty} a_k = 0$. Let $f: [1, \infty) \to \R$ be contiunous
        and decreasing, with $f(k) = a_k$ for all $k \ge 1$. Then

        $\sum_{k = 1}^{\infty} a_k$ converges if and only if $\int_1^{\infty}
        f(x)dx$ converges.


        {\bf $p$ Test}

        $\sum_{k = 1}^{\infty} \frac{1}{k^p}$ converges if and only if $p > 1$
        with $p \in \R$

        {\bf Alternating Convergence Test}

        $\sum_{k = 1}^{\infty} (-1)^k a_k$ is alternating if $a_k \ge 0$ for all
        $k$. If $a_k$ is strictly decreasing, then the alternating series
        converges.


        {\bf Root Test}

        Consider $\sum_{k = 1}^{\infty} a_k$ with $a_k \ge 0$ for all $k$.
        Assume that $\lim_{k \to \infty} (a_k)^{1 / k} = L$

        \begin{itemize}
          \item If $0 \ge L < 1$, then the series converges
          \item If $L > 1$, then the series diverges
          \item If $L = 1$, then the root test doesn't tell us anything
        \end{itemize}


        \subsubsection{Pointwise Convergence}

        \[
            (\forall x \in D)(\forall \eps > 0)(\exists N \in \N)(\forall n \in \N)
            [n > N \Rightarrow f_n(x) \in (f(x) - \eps, f(x) + \eps)]
        \]

        Pick any $x$ in your domain, then pick any margin $\eps > 0$ around
        your domain, then you can go far enough out in your sequence of
        functions (specifically at least $N$ far) so that every $n$ larger than
        $N$ guarantees that the ${f_n}$ function at point $x$ lives inside the
        $\eps$ margin around the true function $f$ at point $x$.

        \subsubsection{Uniform Convergence}

        {\it Uniform convergence is stronger. Everything that converges
        uniformly converges pointwise, but the converse is not necessarily
        true.}

        \[
            (\forall \eps > 0)(\exists N \in \N)(\forall x \in D)(\forall n \in \N)
            [n > N \Rightarrow f_n(x) \in (f(x) - \eps, f(x) + \eps)]
        \]

        Pick any $\eps$ margin, if you go far enough out in your sequence of
        functions (specifically $N$ far) and then pick any $x$ in your domain,
        then for all numbers $n$ larger $N$ it is guaranteed that the ${f_n}$
        function at point $x$ lives inside the $\eps$ margin around the true
        function $f$ at point $x$.

        The distinction is that by picking a margin for the whole interval
        first, and then picking a point $x$ in your domain, your entire
        function has to live in this $\eps$ margin in order to converge
        uniformly.

        On the other hand, for pointwise convergence, the ability to pick a
        distinct $N$ that satisfies the $\eps$ margin for each point $x$ in
        our domain makes it ``easier" for the function $f$ to converge. Note
        that this $\eps$ margin is the same at every point, but the $N$ value
        might differ at every point.

    \end{multicols*}
\end{document}

